{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbymVTMFpqeiC/y8KdKGf0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anna-alt/AI-Lab/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "X6EjCX3RNv7S",
        "outputId": "da19b75e-4671-4dde-89b9-ce274b4259d8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b2e46bdb1a58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'num_workers'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pin_memory'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m train_loader = torch.utils.data.DataLoader(\n\u001b[0;32m---> 49\u001b[0;31m     datasets.Places365('../data', train=True, download=True,\n\u001b[0m\u001b[1;32m     50\u001b[0m                     transform=transforms.Compose([\n\u001b[1;32m     51\u001b[0m                         \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'train'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "\n",
        "class GraphNet(nn.Module):\n",
        "    def __init__(self, image, pred_edge = False):\n",
        "        super(GraphNet, self).__init__()\n",
        "        self.pred_edge = pred_edge\n",
        "        superpixels_labels = slic(image, compactness=30, n_segments=72, multichannel=False) + 1 \n",
        "    def build_rag(labels, image):\n",
        "      g = nx.Graph()\n",
        "      footprint = ndi.generate_binary_structure(labels.ndim, connectivity=1)\n",
        "      _ = ndi.generic_filter(labels, add_edge_filter, footprint=footprint,\n",
        "                            mode='nearest', extra_arguments=(g,))\n",
        "      for n in g:\n",
        "          g.nodes[n]['total color'] = np.zeros(34, np.double)\n",
        "          g.nodes[n]['pixel count'] = 0\n",
        "      for index in np.ndindex(labels.shape):\n",
        "          n = labels[index]\n",
        "          g.nodes[n]['total color'] += image[index]\n",
        "          g.nodes[n]['pixel count'] += 1\n",
        "      return g\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: image (batch_size x 1 x image_width x image_height)\n",
        "        '''\n",
        "        B = x.size(0) # 64\n",
        "        if self.pred_edge:\n",
        "            self.A = self.pred_edge_fc(self.adjacency_matrix).squeeze() # (784 x 784) --> predicted edge map\n",
        "\n",
        "        avg_neighbor_features = (torch.bmm(self.A.unsqueeze(0).expand(B, -1, -1), \n",
        "                                            x.view(B, -1, 1)).view(B, -1)) # (64 X 784)\n",
        "        return self.fc(avg_neighbor_features)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.Places365('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=64, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.Places365('../data', train=False, transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])),\n",
        "    batch_size=64, shuffle=False, **kwargs)     \n",
        "    \n",
        "model = GraphNet()\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 1e-4)\n",
        "\n",
        "print('number of trainable parameters: %d' %\n",
        "      np.sum([np.prod(p.size()) if p.requires_grad else 0 for p in model.parameters()]))\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        # Cross entropy loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 1000 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            \n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(\n",
        "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    \n",
        "\n",
        "for epoch in range(1, 10 + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage\n",
        "from skimage.segmentation import slic\n",
        "import networkx as nx\n",
        "from scipy import ndimage as ndi\n",
        "\n",
        "superpixels_labels = slic(\"/content/test.jpg\", compactness=30, n_segments=72, multichannel=False) + 1 \n",
        "\n",
        "def build_rag(labels, image):\n",
        "    g = nx.Graph()\n",
        "    footprint = ndi.generate_binary_structure(labels.ndim, connectivity=1)\n",
        "    _ = ndi.generic_filter(labels, add_edge_filter, footprint=footprint,\n",
        "                           mode='nearest', extra_arguments=(g,))\n",
        "    for n in g:\n",
        "        g.nodes[n]['total color'] = np.zeros(34, np.double)\n",
        "        g.nodes[n]['pixel count'] = 0\n",
        "    for index in np.ndindex(labels.shape):\n",
        "        n = labels[index]\n",
        "        g.nodes[n]['total color'] += image[index]\n",
        "        g.nodes[n]['pixel count'] += 1\n",
        "    return g\n",
        "\n",
        "\n",
        "#to add node features from image\n",
        "nx.set_node_attributes(g, [0.2, 0.7, 0.5], \"ndata\")\n",
        "g.nodes[1][\"ndata\"]"
      ],
      "metadata": {
        "id": "MDwz9jLmVs5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable as V\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms as trn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        " # hacky way to deal with the Pytorch 1.0 update\n",
        "def recursion_change_bn(module):\n",
        "    if isinstance(module, torch.nn.BatchNorm2d):\n",
        "        module.track_running_stats = 1\n",
        "    else:\n",
        "        for i, (name, module1) in enumerate(module._modules.items()):\n",
        "            module1 = recursion_change_bn(module1)\n",
        "    return module\n",
        "\n",
        "def load_labels():\n",
        "    # prepare all the labels\n",
        "    # scene category relevant\n",
        "    file_name_category = 'categories_places365.txt'\n",
        "    if not os.access(file_name_category, os.W_OK):\n",
        "        synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt'\n",
        "        os.system('wget ' + synset_url)\n",
        "    classes = list()\n",
        "    with open(file_name_category) as class_file:\n",
        "        for line in class_file:\n",
        "            classes.append(line.strip().split(' ')[0][3:])\n",
        "    classes = tuple(classes)\n",
        "\n",
        "    # indoor and outdoor relevant\n",
        "    file_name_IO = 'IO_places365.txt'\n",
        "    if not os.access(file_name_IO, os.W_OK):\n",
        "        synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/IO_places365.txt'\n",
        "        os.system('wget ' + synset_url)\n",
        "    with open(file_name_IO) as f:\n",
        "        lines = f.readlines()\n",
        "        labels_IO = []\n",
        "        for line in lines:\n",
        "            items = line.rstrip().split()\n",
        "            labels_IO.append(int(items[-1]) -1) # 0 is indoor, 1 is outdoor\n",
        "    labels_IO = np.array(labels_IO)\n",
        "\n",
        "    # scene attribute relevant\n",
        "    file_name_attribute = 'labels_sunattribute.txt'\n",
        "    if not os.access(file_name_attribute, os.W_OK):\n",
        "        synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/labels_sunattribute.txt'\n",
        "        os.system('wget ' + synset_url)\n",
        "    with open(file_name_attribute) as f:\n",
        "        lines = f.readlines()\n",
        "        labels_attribute = [item.rstrip() for item in lines]\n",
        "    file_name_W = 'W_sceneattribute_wideresnet18.npy'\n",
        "    if not os.access(file_name_W, os.W_OK):\n",
        "        synset_url = 'http://places2.csail.mit.edu/models_places365/W_sceneattribute_wideresnet18.npy'\n",
        "        os.system('wget ' + synset_url)\n",
        "    W_attribute = np.load(file_name_W)\n",
        "\n",
        "    return classes, labels_IO, labels_attribute, W_attribute\n",
        "\n",
        "def hook_feature(module, input, output):\n",
        "    features_blobs.append(np.squeeze(output.data.cpu().numpy()))\n",
        "\n",
        "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
        "    # generate the class activation maps upsample to 256x256\n",
        "    size_upsample = (256, 256)\n",
        "    nc, h, w = feature_conv.shape\n",
        "    output_cam = []\n",
        "    for idx in class_idx:\n",
        "        cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc, h*w)))\n",
        "        cam = cam.reshape(h, w)\n",
        "        cam = cam - np.min(cam)\n",
        "        cam_img = cam / np.max(cam)\n",
        "        cam_img = np.uint8(255 * cam_img)\n",
        "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
        "    return output_cam\n",
        "\n",
        "def returnTF():\n",
        "# load the image transformer\n",
        "    tf = trn.Compose([\n",
        "        trn.Resize((224,224)),\n",
        "        trn.ToTensor(),\n",
        "        trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    return tf\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    # this model has a last conv feature map as 14x14\n",
        "\n",
        "    model_file = 'wideresnet18_places365.pth.tar'\n",
        "    if not os.access(model_file, os.W_OK):\n",
        "        os.system('wget http://places2.csail.mit.edu/models_places365/' + model_file)\n",
        "        os.system('wget https://raw.githubusercontent.com/csailvision/places365/master/wideresnet.py')\n",
        "\n",
        "    import wideresnet\n",
        "    model = wideresnet.resnet18(num_classes=365)\n",
        "    checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage)\n",
        "    state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
        "    model.load_state_dict(state_dict)\n",
        "    \n",
        "    # hacky way to deal with the upgraded batchnorm2D and avgpool layers...\n",
        "    for i, (name, module) in enumerate(model._modules.items()):\n",
        "        module = recursion_change_bn(model)\n",
        "    model.avgpool = torch.nn.AvgPool2d(kernel_size=14, stride=1, padding=0)\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "\n",
        "\n",
        "    # the following is deprecated, everything is migrated to python36\n",
        "\n",
        "    ## if you encounter the UnicodeDecodeError when use python3 to load the model, add the following line will fix it. Thanks to @soravux\n",
        "    #from functools import partial\n",
        "    #import pickle\n",
        "    #pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
        "    #pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
        "    #model = torch.load(model_file, map_location=lambda storage, loc: storage, pickle_module=pickle)\n",
        "\n",
        "    model.eval()\n",
        "    # hook the feature extractor\n",
        "    features_names = ['layer4','avgpool'] # this is the last conv layer of the resnet\n",
        "    for name in features_names:\n",
        "        model._modules.get(name).register_forward_hook(hook_feature)\n",
        "    return model\n",
        "\n",
        "\n",
        "# load the labels\n",
        "classes, labels_IO, labels_attribute, W_attribute = load_labels()\n",
        "\n",
        "# load the model\n",
        "features_blobs = []\n",
        "model = load_model()\n",
        "\n",
        "# load the transformer\n",
        "tf = returnTF() # image transformer\n",
        "\n",
        "# get the softmax weight\n",
        "params = list(model.parameters())\n",
        "weight_softmax = params[-2].data.numpy()\n",
        "weight_softmax[weight_softmax<0] = 0\n",
        "\n",
        "# load the test image\n",
        "img_url = 'http://places.csail.mit.edu/demo/6.jpg'\n",
        "os.system('wget %s -q -O test.jpg' % img_url)\n",
        "img = Image.open('test.jpg')\n",
        "input_img = V(tf(img).unsqueeze(0))\n",
        "\n",
        "# forward pass\n",
        "logit = model.forward(input_img)\n",
        "h_x = F.softmax(logit, 1).data.squeeze()\n",
        "probs, idx = h_x.sort(0, True)\n",
        "probs = probs.numpy()\n",
        "idx = idx.numpy()\n",
        "\n",
        "print('RESULT ON ' + img_url)\n",
        "\n",
        "# output the IO prediction\n",
        "io_image = np.mean(labels_IO[idx[:10]]) # vote for the indoor or outdoor\n",
        "if io_image < 0.5:\n",
        "    print('--TYPE OF ENVIRONMENT: indoor')\n",
        "else:\n",
        "    print('--TYPE OF ENVIRONMENT: outdoor')\n",
        "\n",
        "# output the prediction of scene category\n",
        "print('--SCENE CATEGORIES:')\n",
        "for i in range(0, 5):\n",
        "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
        "\n",
        "# output the scene attributes\n",
        "responses_attribute = W_attribute.dot(features_blobs[1])\n",
        "idx_a = np.argsort(responses_attribute)\n",
        "print('--SCENE ATTRIBUTES:')\n",
        "print(', '.join([labels_attribute[idx_a[i]] for i in range(-1,-10,-1)]))\n",
        "\n",
        "\n",
        "# generate class activation mapping\n",
        "print('Class activation map is saved as cam.jpg')\n",
        "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
        "\n",
        "# render the CAM and output\n",
        "img = cv2.imread('test.jpg')\n",
        "height, width, _ = img.shape\n",
        "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
        "result = heatmap * 0.4 + img * 0.5\n",
        "cv2.imwrite('cam.jpg', result)"
      ],
      "metadata": {
        "id": "lKt77WJtZFRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b45ccc-c272-4040-a0d7-3ef57547e16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RESULT ON http://places.csail.mit.edu/demo/6.jpg\n",
            "--TYPE OF ENVIRONMENT: indoor\n",
            "--SCENE CATEGORIES:\n",
            "0.511 -> food_court\n",
            "0.085 -> fastfood_restaurant\n",
            "0.083 -> cafeteria\n",
            "0.040 -> dining_hall\n",
            "0.021 -> flea_market/indoor\n",
            "--SCENE ATTRIBUTES:\n",
            "no horizon, enclosed area, man-made, socializing, indoor lighting, cloth, congregating, eating, working\n",
            "Class activation map is saved as cam.jpg\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}