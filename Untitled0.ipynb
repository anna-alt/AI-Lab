{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxQ3lvXPjHfyO84YEM1Qii",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anna-alt/AI-Lab/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "X6EjCX3RNv7S",
        "outputId": "da19b75e-4671-4dde-89b9-ce274b4259d8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b2e46bdb1a58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'num_workers'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pin_memory'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m train_loader = torch.utils.data.DataLoader(\n\u001b[0;32m---> 49\u001b[0;31m     datasets.Places365('../data', train=True, download=True,\n\u001b[0m\u001b[1;32m     50\u001b[0m                     transform=transforms.Compose([\n\u001b[1;32m     51\u001b[0m                         \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'train'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "\n",
        "class GraphNet(nn.Module):\n",
        "    def __init__(self, image, pred_edge = False):\n",
        "        super(GraphNet, self).__init__()\n",
        "        self.pred_edge = pred_edge\n",
        "        superpixels_labels = slic(image, compactness=30, n_segments=72, multichannel=False) + 1 \n",
        "    def build_rag(labels, image):\n",
        "      g = nx.Graph()\n",
        "      footprint = ndi.generate_binary_structure(labels.ndim, connectivity=1)\n",
        "      _ = ndi.generic_filter(labels, add_edge_filter, footprint=footprint,\n",
        "                            mode='nearest', extra_arguments=(g,))\n",
        "      for n in g:\n",
        "          g.nodes[n]['total color'] = np.zeros(34, np.double)\n",
        "          g.nodes[n]['pixel count'] = 0\n",
        "      for index in np.ndindex(labels.shape):\n",
        "          n = labels[index]\n",
        "          g.nodes[n]['total color'] += image[index]\n",
        "          g.nodes[n]['pixel count'] += 1\n",
        "      return g\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: image (batch_size x 1 x image_width x image_height)\n",
        "        '''\n",
        "        B = x.size(0) # 64\n",
        "        if self.pred_edge:\n",
        "            self.A = self.pred_edge_fc(self.adjacency_matrix).squeeze() # (784 x 784) --> predicted edge map\n",
        "\n",
        "        avg_neighbor_features = (torch.bmm(self.A.unsqueeze(0).expand(B, -1, -1), \n",
        "                                            x.view(B, -1, 1)).view(B, -1)) # (64 X 784)\n",
        "        return self.fc(avg_neighbor_features)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.Places365('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=64, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.Places365('../data', train=False, transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])),\n",
        "    batch_size=64, shuffle=False, **kwargs)     \n",
        "    \n",
        "model = GraphNet()\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 1e-4)\n",
        "\n",
        "print('number of trainable parameters: %d' %\n",
        "      np.sum([np.prod(p.size()) if p.requires_grad else 0 for p in model.parameters()]))\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        # Cross entropy loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 1000 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            \n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(\n",
        "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    \n",
        "\n",
        "for epoch in range(1, 10 + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage\n",
        "from skimage.segmentation import slic\n",
        "import networkx as nx\n",
        "from scipy import ndimage as ndi\n",
        "\n",
        "superpixels_labels = slic(, compactness=30, n_segments=72, multichannel=False) + 1 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_rag(labels, image):\n",
        "    g = nx.Graph()\n",
        "    footprint = ndi.generate_binary_structure(labels.ndim, connectivity=1)\n",
        "    _ = ndi.generic_filter(labels, add_edge_filter, footprint=footprint,\n",
        "                           mode='nearest', extra_arguments=(g,))\n",
        "    for n in g:\n",
        "        g.nodes[n]['total color'] = np.zeros(34, np.double)\n",
        "        g.nodes[n]['pixel count'] = 0\n",
        "    for index in np.ndindex(labels.shape):\n",
        "        n = labels[index]\n",
        "        g.nodes[n]['total color'] += image[index]\n",
        "        g.nodes[n]['pixel count'] += 1\n",
        "    return g\n",
        "\n",
        "\n",
        "#to add node features from image\n",
        "nx.set_node_attributes(g, [0.2, 0.7, 0.5], \"ndata\")\n",
        "g.nodes[1][\"ndata\"]"
      ],
      "metadata": {
        "id": "MDwz9jLmVs5V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "83c33ccc-e723-48f4-8c9e-4b2d693af4ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2668ae10005e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndimage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mndi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msuperpixels_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/test.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompactness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_segments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m72\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultichannel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_rag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/skimage/segmentation/slic_superpixels.py\u001b[0m in \u001b[0;36mslic\u001b[0;34m(image, n_segments, compactness, max_iter, sigma, spacing, multichannel, convert2lab, enforce_connectivity, min_size_factor, max_size_factor, slic_zero, start_label, mask)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \"\"\"\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_as_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0muse_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/skimage/util/dtype.py\u001b[0m in \u001b[0;36mimg_as_float\u001b[0;34m(image, force_copy)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \"\"\"\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/skimage/util/dtype.py\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(image, dtype, force_copy, uniform)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtype_in\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_supported_types\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype_out\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_supported_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         raise ValueError(\"Can not convert from {} to {}.\"\n\u001b[0m\u001b[1;32m    256\u001b[0m                          .format(dtypeobj_in, dtypeobj_out))\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can not convert from <U17 to float64."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable as V\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms as trn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        " # hacky way to deal with the Pytorch 1.0 update\n",
        "def recursion_change_bn(module):\n",
        "    if isinstance(module, torch.nn.BatchNorm2d):\n",
        "        module.track_running_stats = 1\n",
        "    else:\n",
        "        for i, (name, module1) in enumerate(module._modules.items()):\n",
        "            module1 = recursion_change_bn(module1)\n",
        "    return module\n",
        "\n",
        "def load_labels():\n",
        "    # prepare all the labels\n",
        "    # scene category relevant\n",
        "    file_name_category = 'categories_places365.txt'\n",
        "    if not os.access(file_name_category, os.W_OK):\n",
        "        synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt'\n",
        "        os.system('wget ' + synset_url)\n",
        "    classes = list()\n",
        "    with open(file_name_category) as class_file:\n",
        "        for line in class_file:\n",
        "            classes.append(line.strip().split(' ')[0][3:])\n",
        "    classes = tuple(classes)\n",
        "\n",
        "    # indoor and outdoor relevant\n",
        "    file_name_IO = 'IO_places365.txt'\n",
        "    if not os.access(file_name_IO, os.W_OK):\n",
        "        synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/IO_places365.txt'\n",
        "        os.system('wget ' + synset_url)\n",
        "    with open(file_name_IO) as f:\n",
        "        lines = f.readlines()\n",
        "        labels_IO = []\n",
        "        for line in lines:\n",
        "            items = line.rstrip().split()\n",
        "            labels_IO.append(int(items[-1]) -1) # 0 is indoor, 1 is outdoor\n",
        "    labels_IO = np.array(labels_IO)\n",
        "\n",
        "    # scene attribute relevant\n",
        "    file_name_attribute = 'labels_sunattribute.txt'\n",
        "    if not os.access(file_name_attribute, os.W_OK):\n",
        "        synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/labels_sunattribute.txt'\n",
        "        os.system('wget ' + synset_url)\n",
        "    with open(file_name_attribute) as f:\n",
        "        lines = f.readlines()\n",
        "        labels_attribute = [item.rstrip() for item in lines]\n",
        "    file_name_W = 'W_sceneattribute_wideresnet18.npy'\n",
        "    if not os.access(file_name_W, os.W_OK):\n",
        "        synset_url = 'http://places2.csail.mit.edu/models_places365/W_sceneattribute_wideresnet18.npy'\n",
        "        os.system('wget ' + synset_url)\n",
        "    W_attribute = np.load(file_name_W)\n",
        "\n",
        "    return classes, labels_IO, labels_attribute, W_attribute\n",
        "\n",
        "def hook_feature(module, input, output):\n",
        "    features_blobs.append(np.squeeze(output.data.cpu().numpy()))\n",
        "\n",
        "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
        "    # generate the class activation maps upsample to 256x256\n",
        "    size_upsample = (256, 256)\n",
        "    nc, h, w = feature_conv.shape\n",
        "    output_cam = []\n",
        "    for idx in class_idx:\n",
        "        cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc, h*w)))\n",
        "        cam = cam.reshape(h, w)\n",
        "        cam = cam - np.min(cam)\n",
        "        cam_img = cam / np.max(cam)\n",
        "        cam_img = np.uint8(255 * cam_img)\n",
        "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
        "    return output_cam\n",
        "\n",
        "def returnTF():\n",
        "# load the image transformer\n",
        "    tf = trn.Compose([\n",
        "        trn.Resize((224,224)),\n",
        "        trn.ToTensor(),\n",
        "        trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    return tf\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    # this model has a last conv feature map as 14x14\n",
        "\n",
        "    model_file = 'wideresnet18_places365.pth.tar'\n",
        "    if not os.access(model_file, os.W_OK):\n",
        "        os.system('wget http://places2.csail.mit.edu/models_places365/' + model_file)\n",
        "        os.system('wget https://raw.githubusercontent.com/csailvision/places365/master/wideresnet.py')\n",
        "\n",
        "    import wideresnet\n",
        "    model = wideresnet.resnet18(num_classes=365)\n",
        "    checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage)\n",
        "    state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
        "    model.load_state_dict(state_dict)\n",
        "    \n",
        "    # hacky way to deal with the upgraded batchnorm2D and avgpool layers...\n",
        "    for i, (name, module) in enumerate(model._modules.items()):\n",
        "        module = recursion_change_bn(model)\n",
        "    model.avgpool = torch.nn.AvgPool2d(kernel_size=14, stride=1, padding=0)\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "\n",
        "\n",
        "    # the following is deprecated, everything is migrated to python36\n",
        "\n",
        "    ## if you encounter the UnicodeDecodeError when use python3 to load the model, add the following line will fix it. Thanks to @soravux\n",
        "    #from functools import partial\n",
        "    #import pickle\n",
        "    #pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
        "    #pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
        "    #model = torch.load(model_file, map_location=lambda storage, loc: storage, pickle_module=pickle)\n",
        "\n",
        "    model.eval()\n",
        "    # hook the feature extractor\n",
        "    features_names = ['layer4','avgpool'] # this is the last conv layer of the resnet\n",
        "    for name in features_names:\n",
        "        model._modules.get(name).register_forward_hook(hook_feature)\n",
        "    return model\n",
        "\n",
        "\n",
        "# load the labels\n",
        "classes, labels_IO, labels_attribute, W_attribute = load_labels()\n",
        "\n",
        "# load the model\n",
        "features_blobs = []\n",
        "model = load_model()\n",
        "\n",
        "# load the transformer\n",
        "tf = returnTF() # image transformer\n",
        "\n",
        "# get the softmax weight\n",
        "params = list(model.parameters())\n",
        "weight_softmax = params[-2].data.numpy()\n",
        "weight_softmax[weight_softmax<0] = 0\n",
        "\n",
        "# load the test image\n",
        "img_url = 'http://places.csail.mit.edu/demo/6.jpg'\n",
        "os.system('wget %s -q -O test.jpg' % img_url)\n",
        "img = Image.open('test.jpg')\n",
        "input_img = V(tf(img).unsqueeze(0))\n",
        "\n",
        "# forward pass\n",
        "logit = model.forward(input_img)\n",
        "h_x = F.softmax(logit, 1).data.squeeze()\n",
        "probs, idx = h_x.sort(0, True)\n",
        "probs = probs.numpy()\n",
        "idx = idx.numpy()\n",
        "\n",
        "print('RESULT ON ' + img_url)\n",
        "\n",
        "# output the IO prediction\n",
        "io_image = np.mean(labels_IO[idx[:10]]) # vote for the indoor or outdoor\n",
        "if io_image < 0.5:\n",
        "    print('--TYPE OF ENVIRONMENT: indoor')\n",
        "else:\n",
        "    print('--TYPE OF ENVIRONMENT: outdoor')\n",
        "\n",
        "# output the prediction of scene category\n",
        "print('--SCENE CATEGORIES:')\n",
        "for i in range(0, 5):\n",
        "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
        "\n",
        "# output the scene attributes\n",
        "responses_attribute = W_attribute.dot(features_blobs[1])\n",
        "idx_a = np.argsort(responses_attribute)\n",
        "print('--SCENE ATTRIBUTES:')\n",
        "print(', '.join([labels_attribute[idx_a[i]] for i in range(-1,-10,-1)]))\n",
        "\n",
        "\n",
        "# generate class activation mapping\n",
        "\n",
        "\n",
        "# render the CAM and output\n",
        "img = cv2.imread('test.jpg')\n",
        "height, width, _ = img.shape\n",
        "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
        "result = heatmap * 0.4 + img * 0.5\n",
        "cv2.imwrite('cam.jpg', result)"
      ],
      "metadata": {
        "id": "lKt77WJtZFRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import skimage\n",
        "from skimage.segmentation import slic\n",
        "import networkx as nx\n",
        "from scipy import ndimage as ndi\n",
        "import torch\n",
        "\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "!pip install pyg-lib\n",
        "\n",
        "!pip install pyg-lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
        "!pip install torch_geometric\n",
        "\n",
        "\n",
        "from torch_geometric.nn import global_mean_pool,graclus\n",
        "from torch_spline_conv import spline_conv\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn.functional as fun\n",
        "class fcmodel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(fcmodel, self).__init__()\n",
        "        self.conv = spline_conv((3,3), 1,32)\n",
        "        self.pool  = graclus(self.conv.edge_index())\n",
        "        self.conv1 = spline_conv((3,3), 32, 64)\n",
        "        self.pool1 = graclus(self.conv1.edge_index())\n",
        "        self.poolmean = global_mean_pool(self.pool1)\n",
        "        self.fc  = nn.Linear(128)\n",
        "        self.fc1   = nn.Linear(10)\n",
        "\n",
        "    def forward(self, y):\n",
        "        y = self.pool(fun.relu(self.conv(y)))\n",
        "        y = self.pool(fun.relu(self.conv1(y)))\n",
        "        y = y.view(-1, 16*5*5)\n",
        "        y = fun.relu(self.fc(y))\n",
        "        y = fun.relu(self.fc1(y))\n",
        "        y = self.fc2(y)\n",
        "        return y\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=64, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])),\n",
        "    batch_size=64, shuffle=False, **kwargs)   \n",
        "\n",
        "model = fcmodel()\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 1e-4)\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data) in enumerate(train_loader):\n",
        "        data = data.to(device), superpixels_labels.to(device)\n",
        "        superpixels_labels = slic(data, compactness=30, n_segments=72, multichannel=False) + 1  \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        # Cross entropy loss\n",
        "        loss = F.cross_entropy(output,superpixels_labels )\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 1000 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            superpixels_labels = slic(data, compactness=30, n_segments=72, multichannel=False) + 1  \n",
        "            output = model(data)\n",
        "            \n",
        "            test_loss += F.cross_entropy(output, superpixels_labels, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(superpixels_labels.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(\n",
        "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "# forward pass\n",
        "\n",
        "\n",
        "for epoch in range(1, 75):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "7sHX_znenmHo",
        "outputId": "603d1ae0-3976-4560-8e4b-9cebbeb499a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyg-lib in /usr/local/lib/python3.8/dist-packages (0.1.0+pt113cu116)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
            "Requirement already satisfied: pyg-lib in /usr/local/lib/python3.8/dist-packages (0.1.0+pt113cu116)\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.8/dist-packages (2.1.0)\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.8/dist-packages (0.6.15+pt113cu116)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch_sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->torch_sparse) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.8/dist-packages (2.2.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.8/dist-packages (from torch_geometric) (5.9.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch_geometric) (4.64.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch_geometric) (2.11.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from torch_geometric) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch_geometric) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torch_geometric) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch_geometric) (1.7.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch_geometric) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torch_geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torch_geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torch_geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torch_geometric) (2022.9.24)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f1923db11f74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m     batch_size=64, shuffle=False, **kwargs)   \n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfcmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-f1923db11f74>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfcmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspline_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgraclus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspline_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: spline_conv() Expected a value of type 'Tensor' for argument 'x' but instead found type 'float'.\nPosition: 0\nValue: 3.3\nDeclaration: spline_conv(Tensor x, Tensor edge_index, Tensor pseudo, Tensor weight, Tensor kernel_size, Tensor is_open_spline, int degree=1, bool norm=True, Tensor? root_weight=None, Tensor? bias=None) -> Tensor\nCast error details: Unable to cast 3.3 to Tensor"
          ]
        }
      ]
    }
  ]
}