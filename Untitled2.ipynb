{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anna-alt/AI-Lab/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "r_MSKbWUSgwx",
        "outputId": "50946c54-65a6-456f-c538-f89494e9a1de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precompute_adjacency_images\n",
            "tensor([[ 0.3400, -0.0852, -0.1736, -0.1938,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [-0.0852,  0.2413, -0.0987, -0.1763, -0.1944,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [-0.1736, -0.0987,  0.2207, -0.1015, -0.1768, -0.1946,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [-0.1938, -0.1763, -0.1015,  0.2166, -0.1020, -0.1770, -0.1946,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [ 0.0000, -0.1944, -0.1768, -0.1020,  0.2166, -0.1020, -0.1770, -0.1946,\n",
            "          0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000, -0.1946, -0.1770, -0.1020,  0.2166, -0.1020, -0.1770,\n",
            "         -0.1946,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000, -0.1946, -0.1770, -0.1020,  0.2166, -0.1020,\n",
            "         -0.1770, -0.1946],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1946, -0.1770, -0.1020,  0.2166,\n",
            "         -0.1020, -0.1770],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1946, -0.1770, -0.1020,\n",
            "          0.2166, -0.1020],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1946, -0.1770,\n",
            "         -0.1020,  0.2166]])\n",
            "number of trainable parameters: 7840\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 4.763162\n",
            "\n",
            "Test set: Average loss: 0.6480, Accuracy: 8229/10000 (82%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.736166\n",
            "\n",
            "Test set: Average loss: 0.5232, Accuracy: 8554/10000 (86%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.527304\n",
            "\n",
            "Test set: Average loss: 0.4730, Accuracy: 8682/10000 (87%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.504970\n",
            "\n",
            "Test set: Average loss: 0.4434, Accuracy: 8736/10000 (87%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.461133\n",
            "\n",
            "Test set: Average loss: 0.4240, Accuracy: 8801/10000 (88%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.595991\n",
            "\n",
            "Test set: Average loss: 0.4103, Accuracy: 8826/10000 (88%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.420465\n",
            "\n",
            "Test set: Average loss: 0.3997, Accuracy: 8862/10000 (89%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.268137\n",
            "\n",
            "Test set: Average loss: 0.3913, Accuracy: 8876/10000 (89%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.324580\n",
            "\n",
            "Test set: Average loss: 0.3834, Accuracy: 8914/10000 (89%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.325872\n",
            "\n",
            "Test set: Average loss: 0.3772, Accuracy: 8927/10000 (89%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "\n",
        "class GraphNet(nn.Module):\n",
        "    def __init__(self, image_size = 28, pred_edge = False):\n",
        "        super(GraphNet, self).__init__()\n",
        "        self.pred_edge = pred_edge\n",
        "        N = image_size ** 2 # Number of pixels in the image\n",
        "        self.fc = nn.Linear(N, 10, bias = False)\n",
        "        # Create the adjacency matrix of size (N X N)\n",
        "        if pred_edge:\n",
        "            # Learn the adjacency matrix (learn to predict the edge between any pair of pixels)\n",
        "            col, row = np.meshgrid(np.arange(image_size), np.arange(image_size)) # (28 x 28) Explanation: https://www.geeksforgeeks.org/numpy-meshgrid-function/\n",
        "            coord = np.stack((col, row), axis = 2).reshape(-1, 2)  # (784 x 2)\n",
        "            coord_normalized = (coord - np.mean(coord, axis = 0)) / (np.std(coord, axis = 0) + 1e-5) # Normalize the matrix\n",
        "            coord_normalized = torch.from_numpy(coord_normalized).float() # (784 x 2)\n",
        "            adjacency_matrix = torch.cat((coord_normalized.unsqueeze(0).repeat(N, 1,  1),\n",
        "                                    coord_normalized.unsqueeze(1).repeat(1, N, 1)), dim=2) # (784 x 784 x 4)\n",
        "            self.pred_edge_fc = nn.Sequential(nn.Linear(4, 64),\n",
        "                                              nn.ReLU(), \n",
        "                                              nn.Linear(64, 1),\n",
        "                                              nn.Tanh())\n",
        "            self.register_buffer('adjacency_matrix', adjacency_matrix) # not to be considered a model paramater that is updated during training\n",
        "        else:\n",
        "            # Use a pre-computed adjacency matrix\n",
        "            A = self.precompute_adjacency_images(image_size)\n",
        "            self.register_buffer('A', A) # not to be considered a model paramater that is updated during training\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: image (batch_size x 1 x image_width x image_height)\n",
        "        '''\n",
        "        B = x.size(0) # 64\n",
        "        if self.pred_edge:\n",
        "            self.A = self.pred_edge_fc(self.adjacency_matrix).squeeze() # (784 x 784) --> predicted edge map\n",
        "\n",
        "        avg_neighbor_features = (torch.bmm(self.A.unsqueeze(0).expand(B, -1, -1), \n",
        "                                            x.view(B, -1, 1)).view(B, -1)) # (64 X 784)\n",
        "        return self.fc(avg_neighbor_features)\n",
        "\n",
        "    @staticmethod\n",
        "    # Static method knows nothing about the class and just deals with the parameters.\n",
        "    def precompute_adjacency_images(image_size):\n",
        "        print('precompute_adjacency_images')\n",
        "        col, row = np.meshgrid(np.arange(image_size), np.arange(image_size)) # (28 x 28) Explanation: https://www.geeksforgeeks.org/numpy-meshgrid-function/\n",
        "        coord = np.stack((col, row), axis = 2).reshape(-1, 2) / image_size # (784 x 2) --> normalize\n",
        "        dist = cdist(coord, coord) # compute distance between every pair of pixels\n",
        "        sigma = 0.05 * np.pi # width of the Gaussian (can be a hyperparameter while training a model)\n",
        "        A = np.exp(-dist / sigma ** 2) # adjacency matrix of spatial similarity\n",
        "        A[A < 0.01] = 0 # suppress values less than 0.01\n",
        "        A = torch.from_numpy(A).float()\n",
        "\n",
        "        # Normalization as per (Kipf & Welling, ICLR 2017)\n",
        "        D = A.sum(1)  # nodes degree (N,)\n",
        "        D_hat = (D + 1e-5) ** (-0.5)\n",
        "        A_hat = D_hat.view(-1, 1) * A * D_hat.view(1, -1)  # N,N\n",
        "\n",
        "        # Some additional trick I found to be useful\n",
        "        A_hat[A_hat > 0.0001] = A_hat[A_hat > 0.0001] - 0.2\n",
        "\n",
        "        print(A_hat[:10, :10])\n",
        "        return A_hat\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=64, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])),\n",
        "    batch_size=64, shuffle=False, **kwargs)     \n",
        "    \n",
        "model = GraphNet()\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-4, weight_decay = 1e-4)\n",
        "\n",
        "print('number of trainable parameters: %d' %\n",
        "      np.sum([np.prod(p.size()) if p.requires_grad else 0 for p in model.parameters()]))\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        # Cross entropy loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 1000 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            \n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(\n",
        "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    \n",
        "\n",
        "for epoch in range(1, 10 + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "\n"
      ]
    }
  ]
}