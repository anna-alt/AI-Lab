{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anna-alt/AI-Lab/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r_MSKbWUSgwx",
        "outputId": "dae08327-fd49-43e4-c94b-eccc02561cbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
            "                             [--lr LR] [--pred_edge] [--seed SEED]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-ed798791-adb8-4678-9021-18e1650f3e07.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "\n",
        "class GraphNet(nn.Module):\n",
        "    def __init__(self, image_size = 28, pred_edge = False):\n",
        "        super(GraphNet, self).__init__()\n",
        "        self.pred_edge = pred_edge\n",
        "        N = image_size ** 2 # Number of pixels in the image\n",
        "        self.fc = nn.Linear(N, 10, bias = False)\n",
        "        # Create the adjacency matrix of size (N X N)\n",
        "        if pred_edge:\n",
        "            # Learn the adjacency matrix (learn to predict the edge between any pair of pixels)\n",
        "            col, row = np.meshgrid(np.arange(image_size), np.arange(image_size)) # (28 x 28) Explanation: https://www.geeksforgeeks.org/numpy-meshgrid-function/\n",
        "            coord = np.stack((col, row), axis = 2).reshape(-1, 2)  # (784 x 2)\n",
        "            coord_normalized = (coord - np.mean(coord, axis = 0)) / (np.std(coord, axis = 0) + 1e-5) # Normalize the matrix\n",
        "            coord_normalized = torch.from_numpy(coord_normalized).float() # (784 x 2)\n",
        "            adjacency_matrix = torch.cat((coord_normalized.unsqueeze(0).repeat(N, 1,  1),\n",
        "                                    coord_normalized.unsqueeze(1).repeat(1, N, 1)), dim=2) # (784 x 784 x 4)\n",
        "            self.pred_edge_fc = nn.Sequential(nn.Linear(4, 64),\n",
        "                                              nn.ReLU(), \n",
        "                                              nn.Linear(64, 1),\n",
        "                                              nn.Tanh())\n",
        "            self.register_buffer('adjacency_matrix', adjacency_matrix) # not to be considered a model paramater that is updated during training\n",
        "        else:\n",
        "            # Use a pre-computed adjacency matrix\n",
        "            A = self.precompute_adjacency_images(image_size)\n",
        "            self.register_buffer('A', A) # not to be considered a model paramater that is updated during training\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: image (batch_size x 1 x image_width x image_height)\n",
        "        '''\n",
        "        B = x.size(0) # 64\n",
        "        if self.pred_edge:\n",
        "            self.A = self.pred_edge_fc(self.adjacency_matrix).squeeze() # (784 x 784) --> predicted edge map\n",
        "\n",
        "        avg_neighbor_features = (torch.bmm(self.A.unsqueeze(0).expand(B, -1, -1), \n",
        "                                            x.view(B, -1, 1)).view(B, -1)) # (64 X 784)\n",
        "        return self.fc(avg_neighbor_features)\n",
        "\n",
        "    @staticmethod\n",
        "    # Static method knows nothing about the class and just deals with the parameters.\n",
        "    def precompute_adjacency_images(image_size):\n",
        "        print('precompute_adjacency_images')\n",
        "        col, row = np.meshgrid(np.arange(image_size), np.arange(image_size)) # (28 x 28) Explanation: https://www.geeksforgeeks.org/numpy-meshgrid-function/\n",
        "        coord = np.stack((col, row), axis = 2).reshape(-1, 2) / image_size # (784 x 2) --> normalize\n",
        "        dist = cdist(coord, coord) # compute distance between every pair of pixels\n",
        "        sigma = 0.05 * np.pi # width of the Gaussian (can be a hyperparameter while training a model)\n",
        "        A = np.exp(-dist / sigma ** 2) # adjacency matrix of spatial similarity\n",
        "        A[A < 0.01] = 0 # suppress values less than 0.01\n",
        "        A = torch.from_numpy(A).float()\n",
        "\n",
        "        # Normalization as per (Kipf & Welling, ICLR 2017)\n",
        "        D = A.sum(1)  # nodes degree (N,)\n",
        "        D_hat = (D + 1e-5) ** (-0.5)\n",
        "        A_hat = D_hat.view(-1, 1) * A * D_hat.view(1, -1)  # N,N\n",
        "\n",
        "        # Some additional trick I found to be useful\n",
        "        A_hat[A_hat > 0.0001] = A_hat[A_hat > 0.0001] - 0.2\n",
        "\n",
        "        print(A_hat[:10, :10])\n",
        "        return A_hat\n",
        "\n",
        "\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        # Cross entropy loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 1000 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(\n",
        "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description = 'GNN PyTorch Example on MNIST')\n",
        "    parser.add_argument('--batch_size', type=int, default=64,\n",
        "                        help='input batch size')\n",
        "    parser.add_argument('--epochs', type=int, default=1,\n",
        "                        help='number of epochs to train')\n",
        "    parser.add_argument('--lr', type=float, default=1e-3,\n",
        "                        help='learning rate')\n",
        "    parser.add_argument('--pred_edge', action='store_true', default=False, \n",
        "                        help='predict edges instead of using predefined ones')\n",
        "    parser.add_argument('--seed', type=int, default=1, \n",
        "                        help='random seed')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    use_cuda = True\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    #device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    device = \"cpu\"\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])),\n",
        "        batch_size=args.batch_size, shuffle=False, **kwargs)    \n",
        "    \n",
        "    model = GraphNet()\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr = args.lr, weight_decay = 1e-4)\n",
        "\n",
        "    print('number of trainable parameters: %d' %\n",
        "          np.sum([np.prod(p.size()) if p.requires_grad else 0 for p in model.parameters()]))\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train(args, model, device, train_loader, optimizer, epoch)\n",
        "        test(args, model, device, test_loader)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}