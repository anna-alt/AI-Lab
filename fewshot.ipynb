{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNac0W1KyJRsAj0o26mU5eq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anna-alt/AI-Lab/blob/main/fewshot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable as V\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms as trn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import torch.nn.functional as F\n",
        "!pip install pyg-lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
        "!pip install torch_geometric\n",
        "from torch_geometric.nn import GCNConv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "TALMD5lEAuqx",
        "outputId": "025f013f-baf7-47ff-b3bf-1334e8cd292b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
            "Collecting pyg-lib\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/pyg_lib-0.1.0%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.0%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.4 MB 49.5 MB/s \n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.15%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch_sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->torch_sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse, torch-scatter, pyg-lib\n",
            "Successfully installed pyg-lib-0.1.0+pt113cu116 torch-scatter-2.1.0+pt113cu116 torch-sparse-0.6.15+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch_geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch_geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch_geometric) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch_geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torch_geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from torch_geometric) (1.0.2)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 48.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch_geometric) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torch_geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torch_geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torch_geometric) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=474514ddd254f75cccc0bc9669a0b823c8c5d3722c2ac38e1b65a80f7da35941\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/a3/20/198928106d3169865ae73afcbd3d3d1796cf6b429b55c65378\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: psutil, torch-geometric\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed psutil-5.9.4 torch-geometric-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Graph_conv_block(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, use_bn=True):\n",
        "        super(Graph_conv_block, self).__init__()\n",
        "\n",
        "        self.weight = nn.Linear(input_dim, output_dim)\n",
        "        if use_bn:\n",
        "            self.bn = nn.BatchNorm1d(output_dim)\n",
        "        else:\n",
        "            self.bn = None\n",
        "\n",
        "    def forward(self, x, A):\n",
        "        x_next = torch.matmul(A, x) # (b, N, input_dim)\n",
        "        x_next = self.weight(x_next) # (b, N, output_dim)\n",
        "\n",
        "        if self.bn is not None:\n",
        "            x_next = torch.transpose(x_next, 1, 2) # (b, output_dim, N)\n",
        "            x_next = x_next.contiguous()\n",
        "            x_next = self.bn(x_next)\n",
        "            x_next = torch.transpose(x_next, 1, 2) # (b, N, output)\n",
        "\n",
        "        return x_next\n",
        "\n",
        "class Adjacency_layer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, ratio=[2,2,1,1]):\n",
        "\n",
        "        super(Adjacency_layer, self).__init__()\n",
        "\n",
        "        module_list = []\n",
        "\n",
        "        for i in range(len(ratio)):\n",
        "            if i == 0:\n",
        "                module_list.append(nn.Conv2d(input_dim, hidden_dim*ratio[i], 1, 1))\n",
        "            else:\n",
        "                module_list.append(nn.Conv2d(hidden_dim*ratio[i-1], hidden_dim*ratio[i], 1, 1))\n",
        "\n",
        "            module_list.append(nn.BatchNorm2d(hidden_dim*ratio[i]))\n",
        "            module_list.append(nn.LeakyReLU())\n",
        "\n",
        "        module_list.append(nn.Conv2d(hidden_dim*ratio[-1], 1, 1, 1))\n",
        "\n",
        "        self.module_list = nn.ModuleList(module_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        X_i = x.unsqueeze(2) # (b, N , 1, input_dim)\n",
        "        X_j = torch.transpose(X_i, 1, 2) # (b, 1, N, input_dim)\n",
        "\n",
        "        phi = torch.abs(X_i - X_j) # (b, N, N, input_dim)\n",
        "\n",
        "        phi = torch.transpose(phi, 1, 3) # (b, input_dim, N, N)\n",
        "\n",
        "        A = phi\n",
        "\n",
        "        for l in self.module_list:\n",
        "            A = l(A)\n",
        "        # (b, 1, N, N)\n",
        "\n",
        "        A = torch.transpose(A, 1, 3) # (b, N, N, 1)\n",
        "\n",
        "        A = F.softmax(A, 2) # normalize\n",
        "\n",
        "        return A.squeeze(3) # (b, N, N)\n",
        "\n",
        "class GNN_module(nn.Module):\n",
        "    def __init__(self, nway, input_dim, hidden_dim, num_layers, feature_type='dense'):\n",
        "        super(GNN_module, self).__init__()\n",
        "\n",
        "        self.feature_type = feature_type\n",
        "\n",
        "        adjacency_list = []\n",
        "        graph_conv_list = []\n",
        "\n",
        "        # ratio = [2, 2, 1, 1]\n",
        "        ratio = [2, 1]\n",
        "\n",
        "        if self.feature_type == 'dense':\n",
        "            for i in range(num_layers):\n",
        "                adjacency_list.append(Adjacency_layer(\n",
        "                    input_dim=input_dim+hidden_dim//2*i, \n",
        "                    hidden_dim=hidden_dim, \n",
        "                    ratio=ratio))\n",
        "\n",
        "                graph_conv_list.append(Graph_conv_block(\n",
        "                    input_dim=input_dim+hidden_dim//2*i, \n",
        "                    output_dim=hidden_dim//2))\n",
        "\n",
        "            # last layer\n",
        "            last_adjacency = Adjacency_layer(\n",
        "                        input_dim=input_dim+hidden_dim//2*num_layers, \n",
        "                        hidden_dim=hidden_dim, \n",
        "                        ratio=ratio)\n",
        "\n",
        "            last_conv = Graph_conv_block(\n",
        "                    input_dim=input_dim+hidden_dim//2*num_layers, \n",
        "                    output_dim=nway, \n",
        "                    use_bn=False)\n",
        "\n",
        "        elif self.feature_type == 'forward':\n",
        "            for i in range(num_layers):\n",
        "                adjacency_list.append(Adjacency_layer(\n",
        "                    input_dim=input_dim if i == 0 else hidden_dim, \n",
        "                    hidden_dim=hidden_dim, \n",
        "                    ratio=ratio))\n",
        "\n",
        "                graph_conv_list.append(Graph_conv_block(\n",
        "                    input_dim=hidden_dim, \n",
        "                    output_dim=hidden_dim))\n",
        "\n",
        "            # last layer\n",
        "            last_adjacency = Adjacency_layer(\n",
        "                        input_dim=hidden_dim, \n",
        "                        hidden_dim=hidden_dim, \n",
        "                        ratio=ratio)\n",
        "\n",
        "            last_conv = Graph_conv_block(\n",
        "                    input_dim=hidden_dim, \n",
        "                    output_dim=nway,\n",
        "                    use_bn=False)\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.adjacency_list = nn.ModuleList(adjacency_list)\n",
        "        self.graph_conv_list = nn.ModuleList(graph_conv_list)\n",
        "        self.last_adjacency = last_adjacency\n",
        "        self.last_conv = last_conv\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, _ in enumerate(self.adjacency_list):\n",
        "            adjacency_layer = self.adjacency_list[i]\n",
        "            conv_block = self.graph_conv_list[i]\n",
        "\n",
        "            A = adjacency_layer(x)\n",
        "\n",
        "            x_next = conv_block(x, A)\n",
        "\n",
        "            x_next = F.leaky_relu(x_next, 0.1)\n",
        "\n",
        "            if self.feature_type == 'dense':\n",
        "                x = torch.cat([x, x_next], dim=2)\n",
        "            elif self.feature_type == 'forward':\n",
        "                x = x_next\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "        \n",
        "        A = self.last_adjacency(x)\n",
        "        out = self.last_conv(x, A)   \n",
        "\n",
        "        return out[:, 0, :]"
      ],
      "metadata": {
        "id": "r939-Yr8EyEd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import skimage.io\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision as tv\n",
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "\n",
        "class self_Dataset(Dataset):\n",
        "    def __init__(self, data, label=None):\n",
        "        super(self_Dataset, self).__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "    def __getitem__(self, index):\n",
        "        data = self.data[index]\n",
        "        # data = np.moveaxis(data, 3, 1)\n",
        "        # data = data.astype(np.float32)\n",
        "\n",
        "        if self.label is not None:\n",
        "            label = self.label[index]\n",
        "            # print(label)\n",
        "            # label = torch.from_numpy(label)\n",
        "            # label = torch.LongTensor([label])\n",
        "            return data, label\n",
        "        else:\n",
        "            return data, 1\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def count_data(data_dict):\n",
        "    num = 0\n",
        "    for key in data_dict.keys():\n",
        "        num += len(data_dict[key])\n",
        "    return num\n",
        "\n",
        "class self_DataLoader(Dataset):\n",
        "    def __init__(self, root, train=True, dataset='cifar100', seed=1, nway=5):\n",
        "        super(self_DataLoader, self).__init__()\n",
        "\n",
        "        self.seed = seed\n",
        "        self.nway = nway\n",
        "        self.num_labels = 100\n",
        "        self.input_channels = 3\n",
        "        self.size = 32\n",
        "\n",
        "        self.transform = tv.transforms.Compose([\n",
        "            tv.transforms.ToTensor(),\n",
        "            tv.transforms.Normalize([0.5071, 0.4866, 0.4409], \n",
        "                [0.2673, 0.2564, 0.2762])\n",
        "            ])\n",
        "\n",
        "        self.full_data_dict, self.few_data_dict = self.load_data(root, train, dataset)\n",
        "\n",
        "        print('full_data_num: %d' % count_data(self.full_data_dict))\n",
        "        print('few_data_num: %d' % count_data(self.few_data_dict))\n",
        "\n",
        "    def load_data(self, root, train, dataset):\n",
        "        if dataset == 'cifar100':\n",
        "            few_selected_label = random.Random(self.seed).sample(range(self.num_labels), self.nway)\n",
        "            print('selected labeled', few_selected_label)\n",
        "\n",
        "            full_data_dict = {}\n",
        "            few_data_dict = {}\n",
        "\n",
        "            d = CIFAR100(root, train=train, download=True)\n",
        "\n",
        "            for i, (data, label) in enumerate(d):\n",
        "\n",
        "                data = self.transform(data)\n",
        "\n",
        "                if label in few_selected_label:\n",
        "                    data_dict = few_data_dict\n",
        "                else:\n",
        "                    data_dict = full_data_dict\n",
        "\n",
        "                if label not in data_dict:\n",
        "                    data_dict[label] = [data]\n",
        "                else:\n",
        "                    data_dict[label].append(data)\n",
        "            print(i + 1)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        return full_data_dict, few_data_dict\n",
        "\n",
        "    def load_batch_data(self, train=True, batch_size=16, nway=5, num_shots=1):\n",
        "        if train:\n",
        "            data_dict = self.full_data_dict\n",
        "        else:\n",
        "            data_dict = self.few_data_dict\n",
        "\n",
        "        x = []\n",
        "        label_y = [] # fake label: from 0 to (nway - 1)\n",
        "        one_hot_y = [] # one hot for fake label\n",
        "        class_y = [] # real label\n",
        "\n",
        "        xi = []\n",
        "        label_yi = []\n",
        "        one_hot_yi = []\n",
        "        \n",
        "\n",
        "        map_label2class = []\n",
        "\n",
        "        ### the format of x, label_y, one_hot_y, class_y is \n",
        "        ### [tensor, tensor, ..., tensor] len(label_y) = batch size\n",
        "        ### the first dimension of tensor = num_shots\n",
        "\n",
        "        for i in range(batch_size):\n",
        "\n",
        "            # sample the class to train\n",
        "            sampled_classes = random.sample(data_dict.keys(), nway)\n",
        "\n",
        "            positive_class = random.randint(0, nway - 1)\n",
        "\n",
        "            label2class = torch.LongTensor(nway)\n",
        "\n",
        "            single_xi = []\n",
        "            single_one_hot_yi = []\n",
        "            single_label_yi = []\n",
        "            single_class_yi = []\n",
        "\n",
        "\n",
        "            for j, _class in enumerate(sampled_classes):\n",
        "                if j == positive_class:\n",
        "                    ### without loss of generality, we assume the 0th \n",
        "                    ### sampled  class is the target class\n",
        "                    sampled_data = random.sample(data_dict[_class], num_shots+1)\n",
        "\n",
        "                    x.append(sampled_data[0])\n",
        "                    label_y.append(torch.LongTensor([j]))\n",
        "\n",
        "                    one_hot = torch.zeros(nway)\n",
        "                    one_hot[j] = 1.0\n",
        "                    one_hot_y.append(one_hot)\n",
        "\n",
        "                    class_y.append(torch.LongTensor([_class]))\n",
        "\n",
        "                    shots_data = sampled_data[1:]\n",
        "                else:\n",
        "                    shots_data = random.sample(data_dict[_class], num_shots)\n",
        "\n",
        "                single_xi += shots_data\n",
        "                single_label_yi.append(torch.LongTensor([j]).repeat(num_shots))\n",
        "                one_hot = torch.zeros(nway)\n",
        "                one_hot[j] = 1.0\n",
        "                single_one_hot_yi.append(one_hot.repeat(num_shots, 1))\n",
        "\n",
        "                label2class[j] = _class\n",
        "\n",
        "            shuffle_index = torch.randperm(num_shots*nway)\n",
        "            xi.append(torch.stack(single_xi, dim=0)[shuffle_index])\n",
        "            label_yi.append(torch.cat(single_label_yi, dim=0)[shuffle_index])\n",
        "            one_hot_yi.append(torch.cat(single_one_hot_yi, dim=0)[shuffle_index])\n",
        "\n",
        "            map_label2class.append(label2class)\n",
        "\n",
        "        return [torch.stack(x, 0), torch.cat(label_y, 0), torch.stack(one_hot_y, 0), \\\n",
        "            torch.cat(class_y, 0), torch.stack(xi, 0), torch.stack(label_yi, 0), \\\n",
        "            torch.stack(one_hot_yi, 0), torch.stack(map_label2class, 0)]\n",
        "\n",
        "    # def load_batch_data(self, train=True, batch_size=16, nway=5, num_shots=1):\n",
        "\n",
        "    #     if train:\n",
        "    #         data_dict = self.full_data_dict\n",
        "    #     else:\n",
        "    #         data_dict = self.few_data_dict\n",
        "\n",
        "    #     x = torch.zeros(batch_size, self.input_channels, self.size, self.size)\n",
        "    #     label_y = torch.LongTensor(batch_size).zero_()\n",
        "    #     one_hot_y = torch.zeros(batch_size, nway)\n",
        "    #     class_y = torch.LongTensor(batch_size).zero_()\n",
        "    #     xi, label_yi, one_hot_yi, class_yi = [], [], [], []\n",
        "\n",
        "    #     for i in range(nway*num_shots):\n",
        "    #         xi.append(torch.zeros(batch_size, self.input_channels, self.size, self.size))\n",
        "    #         label_yi.append(torch.LongTensor(batch_size).zero_())\n",
        "    #         one_hot_yi.append(torch.zeros(batch_size, nway))\n",
        "    #         class_yi.append(torch.LongTensor(batch_size).zero_())\n",
        "\n",
        "    #     # sample data\n",
        "\n",
        "    #     for i in range(batch_size):\n",
        "\n",
        "    #         # sample the class to train\n",
        "    #         sampled_classes = random.sample(data_dict.keys(), nway)\n",
        "\n",
        "    #         positive_class = random.randint(0, nway - 1)\n",
        "\n",
        "    #         indexes_perm = np.random.permutation(nway * num_shots)\n",
        "\n",
        "    #         counter = 0\n",
        "\n",
        "    #         for j, _class in enumerate(sampled_classes):\n",
        "    #             if j == positive_class:\n",
        "    #                 ### without loss of generality, we assume the 0th \n",
        "    #                 ### sampled  class is the target class\n",
        "    #                 sampled_data = random.sample(data_dict[_class], num_shots+1)\n",
        "\n",
        "    #                 x[i] = sampled_data[0]\n",
        "    #                 label_y[i] = j\n",
        "\n",
        "    #                 one_hot_y[i, j] = 1.0\n",
        "\n",
        "    #                 class_y[i] = _class\n",
        "\n",
        "    #                 shots_data = sampled_data[1:]\n",
        "    #             else:\n",
        "    #                 shots_data = random.sample(data_dict[_class], num_shots)\n",
        "\n",
        "    #             for s_i in range(0, len(shots_data)):\n",
        "    #                 xi[indexes_perm[counter]][i] = shots_data[s_i]\n",
        "                    \n",
        "    #                 label_yi[indexes_perm[counter]][i] = j\n",
        "    #                 one_hot_yi[indexes_perm[counter]][i, j] = 1.0\n",
        "    #                 class_yi[indexes_perm[counter]][i] = _class\n",
        "\n",
        "    #                 counter += 1\n",
        "    #     return [x, label_y, one_hot_y, class_y, torch.stack(xi, 1), torch.stack(label_yi, 1), \\\n",
        "    #         torch.stack(one_hot_yi, 1), torch.stack(class_yi, 1)]\n",
        "\n",
        "    def load_tr_batch(self, batch_size=16, nway=5, num_shots=1):\n",
        "        return self.load_batch_data(True, batch_size, nway, num_shots)\n",
        "\n",
        "    def load_te_batch(self, batch_size=16, nway=5, num_shots=1):\n",
        "        return self.load_batch_data(False, batch_size, nway, num_shots)\n",
        "\n",
        "    def get_data_list(self, data_dict):\n",
        "        data_list = []\n",
        "        label_list = []\n",
        "        for i in data_dict.keys():\n",
        "            for data in data_dict[i]:\n",
        "                data_list.append(data)\n",
        "                label_list.append(i)\n",
        "\n",
        "        \n",
        "\n",
        "        random.shuffle(data_list)\n",
        "        random.shuffle(label_list)\n",
        "\n",
        "        return data_list, label_list\n",
        "\n",
        "    def get_full_data_list(self):\n",
        "        return self.get_data_list(self.full_data_dict)\n",
        "\n",
        "    def get_few_data_list(self):\n",
        "        return self.get_data_list(self.few_data_dict)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    D = self_DataLoader('/home/lab5300/Data', True)\n",
        "\n",
        "    [x, label_y, one_hot_y, class_y, xi, label_yi, one_hot_yi, class_yi] = \\\n",
        "        D.load_tr_batch(batch_size=16, nway=5, num_shots=5)\n",
        "    print(x.size(), label_y.size(), one_hot_y.size(), class_y.size())\n",
        "    print(xi.size(), label_yi.size(), one_hot_yi.size(), class_yi.size())\n",
        "\n",
        "    # print(label_y)\n",
        "    # print(one_hot_y)\n",
        "\n",
        "    print(label_yi[0])\n",
        "    print(one_hot_yi[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukK3z_m_FJ6s",
        "outputId": "f1bbcee2-c166-4b30-bbeb-177a9555e1ba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected labeled [17, 72, 97, 8, 32]\n",
            "Files already downloaded and verified\n",
            "50000\n",
            "full_data_num: 47500\n",
            "few_data_num: 2500\n",
            "torch.Size([16, 3, 32, 32]) torch.Size([16]) torch.Size([16, 5]) torch.Size([16])\n",
            "torch.Size([16, 25, 3, 32, 32]) torch.Size([16, 25]) torch.Size([16, 25, 5]) torch.Size([16, 5])\n",
            "tensor([3, 0, 2, 2, 4, 4, 3, 3, 1, 2, 3, 0, 2, 1, 0, 4, 1, 1, 0, 2, 4, 1, 4, 0,\n",
            "        3])\n",
            "tensor([[0., 0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1., 0.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def np2cuda(array):\n",
        "    tensor = torch.from_numpy(array)\n",
        "    if torch.cuda.is_available():\n",
        "        tensor = tensor.cuda()\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def tensor2cuda(tensor):\n",
        "    if torch.cuda.is_available():\n",
        "        tensor = tensor.cuda()\n",
        "    return tensor\n",
        "\n",
        "class myModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(myModel, self).__init__()\n",
        "\n",
        "    def load(self, file_name):\n",
        "        self.load_state_dict(torch.load(file_name, map_location=lambda storage, loc: storage))\n",
        "    def save(self, file_name):\n",
        "        torch.save(self.state_dict(), file_name)\n",
        "\n",
        "###############################################################\n",
        "## Vanilla CNN model, used to extract visual features\n",
        "\n",
        "class EmbeddingCNN(myModel):\n",
        "\n",
        "    def __init__(self, image_size, cnn_feature_size, cnn_hidden_dim, cnn_num_layers):\n",
        "        super(EmbeddingCNN, self).__init__()\n",
        "\n",
        "        module_list = []\n",
        "        dim = cnn_hidden_dim\n",
        "        for i in range(cnn_num_layers):\n",
        "            if i == 0:\n",
        "                module_list.append(nn.Conv2d(3, dim, 3, 1, 1, bias=False))\n",
        "                module_list.append(nn.BatchNorm2d(dim))\n",
        "            else:\n",
        "                module_list.append(nn.Conv2d(dim, dim*2, 3, 1, 1, bias=False))\n",
        "                module_list.append(nn.BatchNorm2d(dim*2))\n",
        "                dim *= 2\n",
        "            module_list.append(nn.MaxPool2d(2))\n",
        "            module_list.append(nn.LeakyReLU(0.1, True))\n",
        "            image_size //= 2\n",
        "        module_list.append(nn.Conv2d(dim, cnn_feature_size, image_size, 1, bias=False))\n",
        "        module_list.append(nn.BatchNorm2d(cnn_feature_size))\n",
        "        module_list.append(nn.LeakyReLU(0.1, True))\n",
        "\n",
        "        self.module_list = nn.ModuleList(module_list)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        for l in self.module_list:\n",
        "            inputs = l(inputs)\n",
        "\n",
        "        outputs = inputs.view(inputs.size(0), -1)\n",
        "        return outputs\n",
        "\n",
        "    def freeze_weight(self):\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = False\n",
        "    \n",
        "class GNN(myModel):\n",
        "    def __init__(self, cnn_feature_size, gnn_feature_size, nway):\n",
        "        super(GNN, self).__init__()\n",
        "\n",
        "        num_inputs = cnn_feature_size + nway\n",
        "        graph_conv_layer = 2\n",
        "        self.gnn_obj = GNN_module(nway=nway, input_dim=num_inputs, \n",
        "            hidden_dim=gnn_feature_size, \n",
        "            num_layers=graph_conv_layer, \n",
        "            feature_type='dense')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        logits = self.gnn_obj(inputs).squeeze(-1)\n",
        "\n",
        "        return logits\n",
        "      \n",
        "class gnnModel(myModel):\n",
        "    def __init__(self, nway = 5):\n",
        "        super(myModel, self).__init__()\n",
        "        image_size = 32\n",
        "        cnn_feature_size = 64\n",
        "        cnn_hidden_dim = 32\n",
        "        cnn_num_layers = 3\n",
        "\n",
        "        gnn_feature_size = 32\n",
        "\n",
        "        self.cnn_feature = EmbeddingCNN(image_size, cnn_feature_size, cnn_hidden_dim, cnn_num_layers)\n",
        "        self.gnn = GNN(cnn_feature_size, gnn_feature_size, nway)\n",
        "\n",
        "    def forward(self, data):\n",
        "        [x, _, _, _, xi, _, one_hot_yi,_] = data\n",
        "\n",
        "        z = self.cnn_feature(x)\n",
        "        zi_s = [self.cnn_feature(xi[:, i, :, :, :]) for i in range(xi.size(1))]\n",
        "\n",
        "        zi_s = torch.stack(zi_s, dim=1)\n",
        "\n",
        "\n",
        "        # follow the paper, concatenate the information of labels to input features\n",
        "        uniform_pad = torch.FloatTensor(one_hot_yi.size(0), 1, one_hot_yi.size(2)).fill_(\n",
        "            1.0/one_hot_yi.size(2))\n",
        "        uniform_pad = tensor2cuda(uniform_pad)\n",
        "\n",
        "        labels = torch.cat([uniform_pad, one_hot_yi], dim=1)\n",
        "        features = torch.cat([z.unsqueeze(1), zi_s], dim=1)\n",
        "\n",
        "        nodes_features = torch.cat([features, labels], dim=2)\n",
        "\n",
        "        out_logits = self.gnn(inputs=nodes_features)\n",
        "        logsoft_prob = F.log_softmax(out_logits, dim=1)\n",
        "\n",
        "        return logsoft_prob\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self, trainer_dict):\n",
        "\n",
        "        self.num_labels = 100\n",
        "\n",
        "        \n",
        "        self.tr_dataloader = trainer_dict['tr_dataloader']\n",
        "\n",
        "        \n",
        "        Model = gnnModel\n",
        "        \n",
        "        self.model = Model(nway=20)\n",
        "\n",
        "        self.total_iter = 0\n",
        "        self.sample_size = 32\n",
        "\n",
        "    def load_model(self, model_dir):\n",
        "        self.model.load(model_dir)\n",
        "\n",
        "        print('load model sucessfully...')\n",
        "\n",
        "    def load_pretrain(self, model_dir):\n",
        "        self.model.cnn_feature.load(model_dir)\n",
        "\n",
        "        print('load pretrain feature sucessfully...')\n",
        "    \n",
        "    def model_cuda(self):\n",
        "        if torch.cuda.is_available():\n",
        "            self.model.cuda()\n",
        "\n",
        "    def eval(self, dataloader, test_sample):\n",
        "        self.model.eval()\n",
        "        iteration = int(test_sample/16)\n",
        "\n",
        "        total_loss = 0.0\n",
        "        total_sample = 0\n",
        "        total_correct = 0\n",
        "        with torch.no_grad():\n",
        "            for i in range(iteration):\n",
        "                data = dataloader.load_te_batch(batch_size=16, \n",
        "                    nway=20, num_shots=5)\n",
        "\n",
        "                data_cuda = [tensor2cuda(_data) for _data in data]\n",
        "\n",
        "                logsoft_prob = self.model(data_cuda)\n",
        "\n",
        "                label = data_cuda[1]\n",
        "                loss = F.nll_loss(logsoft_prob, label)\n",
        "\n",
        "                total_loss += loss.item() * logsoft_prob.shape[0]\n",
        "\n",
        "                pred = torch.argmax(logsoft_prob, dim=1)\n",
        "\n",
        "                # print(pred)\n",
        "\n",
        "                # print(torch.eq(pred, label).float().sum().item())\n",
        "                # print(label)\n",
        "\n",
        "                assert pred.shape == label.shape\n",
        "\n",
        "                total_correct += torch.eq(pred, label).float().sum().item()\n",
        "                total_sample += pred.shape[0]\n",
        "        print('correct: %d / %d' % (total_correct, total_sample))\n",
        "        print(total_correct)\n",
        "        return total_loss / total_sample, 100.0 * total_correct / total_sample\n",
        "\n",
        "    def train_batch(self):\n",
        "        self.model.train()\n",
        "        \n",
        "\n",
        "        data = self.tr_dataloader.load_tr_batch(batch_size=16, \n",
        "            nway=20, num_shots=5)\n",
        "\n",
        "        data_cuda = [tensor2cuda(_data) for _data in data]\n",
        "\n",
        "        self.opt.zero_grad()\n",
        "\n",
        "        logsoft_prob = self.model(data_cuda)\n",
        "\n",
        "        # print('pred', torch.argmax(logsoft_prob, dim=1))\n",
        "        # print('label', data[2])\n",
        "        label = data_cuda[1]\n",
        "\n",
        "        loss = F.nll_loss(logsoft_prob, label)\n",
        "        loss.backward()\n",
        "        self.opt.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        best_loss = 1e8\n",
        "        best_acc = 0.0\n",
        "        stop = 0\n",
        "        eval_sample = 5000\n",
        "        self.model_cuda()\n",
        "        \n",
        "\n",
        "        self.opt = torch.optim.Adam(\n",
        "            filter(lambda p: p.requires_grad, self.model.parameters()), \n",
        "            lr=1e-2,\n",
        "            weight_decay=1e-6)\n",
        "        # self.opt = torch.optim.Adam(self.model.parameters(), lr=self.args.lr, \n",
        "        #     weight_decay=1e-6)\n",
        "\n",
        "        start = time()\n",
        "        tr_loss_list = []\n",
        "        for i in range(100000):\n",
        "            \n",
        "            tr_loss = self.train_batch()\n",
        "            tr_loss_list.append(tr_loss)\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                del tr_loss_list[:]\n",
        "                start = time()  \n",
        "\n",
        "            if i % 2000 == 0:\n",
        "                va_loss, va_acc = self.eval(self.tr_dataloader, eval_sample)\n",
        "\n",
        "                if va_loss < best_loss:\n",
        "                    stop = 0\n",
        "                    best_loss = va_loss\n",
        "                    best_acc = va_acc\n",
        "\n",
        "                stop += 1\n",
        "                start = time()\n",
        "            \n",
        "                if stop > 5:\n",
        "                    break\n",
        "\n",
        "            self.total_iter += 1\n",
        "\n",
        "\n",
        "    def test(self, test_data_array, te_dataloader):\n",
        "        self.model_cuda()\n",
        "        self.model.eval()\n",
        "        start = 0\n",
        "        end = 0\n",
        "\n",
        "        batch_size = 16\n",
        "        pred_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            while start < test_data_array.shape[0]:\n",
        "                end = start + batch_size \n",
        "                if end >= test_data_array.shape[0]:\n",
        "                    batch_size = test_data_array.shape[0] - start\n",
        "\n",
        "                data = te_dataloader.load_te_batch(batch_size=batch_size, nway=20, \n",
        "                    num_shots=5)\n",
        "\n",
        "                test_x = test_data_array[start:end]\n",
        "\n",
        "                data[0] = np2cuda(test_x)\n",
        "\n",
        "                data_cuda = [tensor2cuda(_data) for _data in data]\n",
        "\n",
        "                map_label2class = data[-1].cpu().numpy()\n",
        "\n",
        "                logsoft_prob = self.model(data_cuda)\n",
        "                # print(logsoft_prob)\n",
        "                pred = torch.argmax(logsoft_prob, dim=1).cpu().numpy()\n",
        "\n",
        "                pred = map_label2class[range(len(pred)), pred]\n",
        "\n",
        "                pred_list.append(pred)\n",
        "\n",
        "                start = end\n",
        "\n",
        "        return np.hstack(pred_list)\n",
        "\n",
        "    def pretrain_eval(self, loader, cnn_feature, classifier):\n",
        "        total_loss = 0 \n",
        "        total_sample = 0\n",
        "        total_correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for j, (data, label) in enumerate(loader):\n",
        "                data = tensor2cuda(data)\n",
        "                label = tensor2cuda(label)\n",
        "                output = classifier(cnn_feature(data))\n",
        "                output = F.log_softmax(output, dim=1)\n",
        "                loss = F.nll_loss(output, label)\n",
        "\n",
        "                total_loss += loss.item() * output.shape[0]\n",
        "\n",
        "                pred = torch.argmax(output, dim=1)\n",
        "\n",
        "                assert pred.shape == label.shape\n",
        "\n",
        "                total_correct += torch.eq(pred, label).float().sum().item()\n",
        "                total_sample += pred.shape[0]\n",
        "\n",
        "        return total_loss / total_sample, 100.0 * total_correct / total_sample\n",
        "\n",
        "    def pretrain(self, pretrain_dataset, test_dataset):\n",
        "        pretrain_loader = torch.utils.data.DataLoader(pretrain_dataset, \n",
        "                batch_size=16, shuffle=True)\n",
        "        test_loader = torch.utils.data.DataLoader(test_dataset, \n",
        "                        batch_size=16, shuffle=True)\n",
        "\n",
        "        self.model_cuda()\n",
        "\n",
        "        best_loss = 1e8\n",
        "        \n",
        "\n",
        "        cnn_feature = self.model.cnn_feature\n",
        "        classifier = nn.Linear(list(cnn_feature.parameters())[-3].shape[0], self.num_labels)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            classifier.cuda()\n",
        "        self.pretrain_opt =  torch.optim.Adam(\n",
        "            list(cnn_feature.parameters()) + list(classifier.parameters()), \n",
        "            lr=1e-2, \n",
        "            weight_decay=1e-6)\n",
        "\n",
        "        start = time()\n",
        "\n",
        "        for i in range(10000):\n",
        "            total_tr_loss = []\n",
        "            for j, (data, label) in enumerate(pretrain_loader):\n",
        "                data = tensor2cuda(data)\n",
        "                label = tensor2cuda(label)\n",
        "                output = classifier(cnn_feature(data))\n",
        "\n",
        "                output = F.log_softmax(output, dim=1)\n",
        "                loss = F.nll_loss(output, label)\n",
        "\n",
        "                self.pretrain_opt.zero_grad()\n",
        "                loss.backward()\n",
        "                self.pretrain_opt.step()\n",
        "                total_tr_loss.append(loss.item())\n",
        "\n",
        "            te_loss, te_acc = self.pretrain_eval(test_loader, cnn_feature, classifier)\n",
        "\n",
        "            if te_loss < best_loss:\n",
        "                stop = 0\n",
        "                best_loss = te_loss\n",
        "                \n",
        "                \n",
        "\n",
        "            stop += 1\n",
        "            start = time()\n",
        "        \n",
        "            if stop > 5:\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "b_s = 10\n",
        "nway = 5\n",
        "shots = 5\n",
        "batch_x = torch.rand(b_s, 3, 32, 32)\n",
        "batches_xi = [torch.rand(b_s, 3, 32, 32) for i in range(nway*shots)]\n",
        "\n",
        "label_x = torch.rand(b_s, nway)\n",
        "\n",
        "labels_yi = [torch.rand(b_s, nway) for i in range(nway*shots)]\n",
        "\n",
        "print('create model...')\n",
        "model = gnnModel(128)\n",
        "\n",
        "#print(model([batch_x, label_x, None, None, batches_xi, labels_yi, None]).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aaXbOGYGLp3",
        "outputId": "d8321c15-610a-4ccc-8e9a-4712bc364c5a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tr_dataloader = self_DataLoader('data', \n",
        "    train=True, dataset='cifar100', seed=1, nway=20)\n",
        "\n",
        "trainer_dict = {'tr_dataloader': tr_dataloader}\n",
        "\n",
        "trainer = Trainer(trainer_dict)\n",
        "\n",
        "    ###########################################\n",
        "    ## pretrain CNN embedding\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "#pretr_tr_data, pretr_tr_label = tr_dataloader.get_full_data_list() # already shuffled the data\n",
        "\n",
        "#va_size = int(0.1 * len(pretr_tr_data))\n",
        "\n",
        "#pretr_tr_dataset = self_Dataset(pretr_tr_data[va_size:], pretr_tr_label[va_size:])\n",
        "#pretr_va_dataset = self_Dataset(pretr_tr_data[:va_size], pretr_tr_label[:va_size])\n",
        "\n",
        "\n",
        "\n",
        "#trainer.pretrain(pretr_tr_dataset, pretr_va_dataset)\n",
        "\n",
        "\n",
        "\n",
        "    ###########################################\n",
        "    ## load the model trained before\n",
        "\n",
        "    \n",
        "    #model_path = os.path.join('', 'model.pth')\n",
        "    #trainer.load_model(model_path)\n",
        "\n",
        "    ###########################################\n",
        "    ## start training\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "0YlaDeNXNUz2",
        "outputId": "fd8b074e-0eaa-4cae-ef52-d3db2a954003"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected labeled [17, 72, 97, 8, 32, 15, 63, 57, 60, 83, 48, 26, 12, 62, 3, 49, 55, 77, 98, 0]\n",
            "Files already downloaded and verified\n",
            "50000\n",
            "full_data_num: 40000\n",
            "few_data_num: 10000\n",
            "correct: 229 / 4992\n",
            "229.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-97e032dd0a6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m## start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-d5a8fffdb920>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0mtr_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-d5a8fffdb920>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogsoft_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "te_dataloader = self_DataLoader('data', \n",
        "    train=False, dataset='cifar100', seed=1, nway=20)\n",
        "\n",
        "\n",
        "test_data_list, test_label_list = te_dataloader.get_few_data_list()\n",
        "\n",
        "test_data_array, test_label_array = np.stack(test_data_list), np.hstack(test_label_list)\n",
        "\n",
        "\n",
        "test_pred = trainer.test(test_data_array, te_dataloader)\n",
        "\n",
        "print(test_pred.shape, test_label_array.shape)\n",
        "\n",
        "correct = (test_pred == test_label_array).sum()\n",
        "test_acc = (test_pred == test_label_array).mean() * 100.0\n",
        "\n",
        "print('test_acc: %.4f %%, correct: %d / %d' % (test_acc, correct, len(test_label_array)))\n",
        "\n"
      ],
      "metadata": {
        "id": "dIiabtl4NTzB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}